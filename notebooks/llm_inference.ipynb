{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396502bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Google Colab, install required packages\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b04584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747783e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model name\n",
    "DEFAULT_MODEL_NAME = \"deepseek-ai/deepseek-llm-7b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e441fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for loaded models and tokenizers\n",
    "_model_cache = {}\n",
    "\n",
    "def get_model_and_tokenizer(model_name):\n",
    "    if model_name not in _model_cache:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "        )\n",
    "        _model_cache[model_name] = (model, tokenizer)\n",
    "    return _model_cache[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb22f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model_name=DEFAULT_MODEL_NAME, max_new_tokens=1000, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate a response from a language model given a prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt.\n",
    "        model_name (str): The HuggingFace model name. Defaults to DEFAULT_MODEL_NAME.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        temperature (float): Sampling temperature. Higher values mean more random generations. Default is 0.7.\n",
    "        top_p (float): Nucleus sampling probability. Lower values mean more focused generations. Default is 0.9.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    model, tokenizer = get_model_and_tokenizer(model_name)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91cdf16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a432d0567a444a18d3acd38fdc817b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are w2vec and BERT different?\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a deep learning-based language representation model that has revolutionized natural language processing (NLP). It uses a self-attention mechanism to understand the context of words in a sentence. On the other hand, W2VEC is a word embedding model that converts words into vectors, which can be used for various NLP tasks like sentiment analysis, text classification, etc.\n",
      "\n",
      "So, the key differences between BERT and W2VEC are:\n",
      "\n",
      "1. W2VEC:\n",
      "\n",
      "- It is a word embedding model.\n",
      "- It converts words into vectors.\n",
      "- It is used for NLP tasks like sentiment analysis, text classification, etc.\n",
      "- It uses a unidirectional approach to convert words into vectors.\n",
      "\n",
      "2. BERT:\n",
      "\n",
      "- It is a deep learning-based language representation model.\n",
      "- It uses a self-attention mechanism to understand the context of words in a sentence.\n",
      "- It is used for various NLP tasks like text classification, question-answering, etc.\n",
      "- It uses a bidirectional approach to understand the context of words in a sentence.\n",
      "\n",
      "In summary, BERT and W2VEC are different in terms of their approach and purpose. W2VEC is a word embedding model that converts words into vectors, while BERT is a deep learning-based language representation model that uses a self-attention mechanism to understand the context of words in a sentence.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "example_prompt = \"How are w2vec and BERT different?\"\n",
    "print(generate_response(example_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
